{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqUXsyzRTxY4"
   },
   "source": [
    "# Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQDu5BtldcFG"
   },
   "outputs": [],
   "source": [
    "!pip install redis faiss-cpu numpy torch transformers openai\n",
    "!sudo apt-get install redis-server\n",
    "!redis-server --daemonize yes\n",
    "!pip install langchain_community tiktoken\n",
    "!pip install together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDKhJwv2T2do"
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_m1uFGXd3qY"
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "import faiss\n",
    "import heapq\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Tuple, Any\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import tiktoken\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "from together import Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PnKUWAMT6Vs"
   },
   "source": [
    "# Cache Base Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2Cn3I1vcxok"
   },
   "outputs": [],
   "source": [
    "class CacheBase:\n",
    "    def __init__(self, redis_host: str = 'localhost', redis_port: int = 6379):\n",
    "        self.redis_client = redis.Redis(host=redis_host, port=redis_port)\n",
    "\n",
    "    def set(self, key: str, value: str):\n",
    "        self.redis_client.set(key, value)\n",
    "\n",
    "    def get(self, key: str) -> str:\n",
    "        return self.redis_client.get(key)\n",
    "\n",
    "    def delete(self, key: str):\n",
    "        self.redis_client.delete(key)\n",
    "\n",
    "    def clear(self):\n",
    "        self.redis_client.flushdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuGBCbJhUCKX"
   },
   "source": [
    "# Vector Base Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rq90B3HJl_QO"
   },
   "outputs": [],
   "source": [
    "class VectorBase:\n",
    "    def __init__(self, dimension: int):\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner product is equivalent to cosine similarity for normalized vectors\n",
    "\n",
    "    def add(self, vectors: np.ndarray):\n",
    "        self.index.add(vectors)\n",
    "\n",
    "    def search(self, query: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return self.index.search(query, k)\n",
    "\n",
    "    def clear(self):\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w-6KSUnUIDX"
   },
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sIxojE3mCUd"
   },
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.dimension = self.model.config.hidden_size\n",
    "\n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "        return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)  # Normalize for cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xRdwjjgULr-"
   },
   "source": [
    "# Large Language Model(LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRdO_fJqeSPu"
   },
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self):\n",
    "        self.client = Together(api_key=\"\")\n",
    "\n",
    "\n",
    "    def generate(self, user_prompt: str) -> str:\n",
    "      return self.client.chat.completions.create(\n",
    "            model=\"meta-llama/Meta-Llama-3-8B-Instruct-Lite\",\n",
    "            messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFL6JcgbUSVD"
   },
   "source": [
    "# Cache Manager\n",
    "Invloves two main components: cache base and vector base storages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IddLnfMkkvd2"
   },
   "outputs": [],
   "source": [
    "class CacheManager:\n",
    "    def __init__(self, cache_base: CacheBase, vector_base: VectorBase,\n",
    "                 embedder: Embedder, llm: LLM, cache_size=10000, save=True):\n",
    "        self.cache_base = cache_base\n",
    "        self.vector_base = vector_base\n",
    "        self.cache_size = cache_size\n",
    "        self.embedder = embedder\n",
    "        self.save = save\n",
    "        self.counter = 0\n",
    "        self.saved_tokens = [0, 0]\n",
    "        self.llm = llm\n",
    "        self.total_hits = 0\n",
    "        self.patterns = []  # To store identified clusters\n",
    "        self.patterns_meta = []\n",
    "        self.Ts = 100000\n",
    "        self.Tp = 0.1\n",
    "        self.min_samples = 3\n",
    "        self.eps = 1\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "    def add_to_cache(self, query: str, response: str):\n",
    "        self.cache_base.set(query, response)\n",
    "        embedding = self.embedder.embed([query])\n",
    "        self.vector_base.add(embedding)\n",
    "        self.cache_base.set(f\"faiss_index:{self.counter}\", query)\n",
    "        self.counter += 1\n",
    "\n",
    "    def clear(self):\n",
    "        self.cache_base.clear()\n",
    "        self.vector_base.clear()\n",
    "        self.counter = 0\n",
    "        self.total_hits = 0\n",
    "        self.saved_tokens = [0, 0]\n",
    "\n",
    "    def __get_token_count(self, text: str) -> int:\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode('utf-8')\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        return len(tokens)\n",
    "\n",
    "    def get_from_cache(self, query: str, similarity_threshold: float = 0.9) -> str:\n",
    "        query_embedding = self.embedder.embed([query])\n",
    "        distances, indices = self.vector_base.search(query_embedding, 1)\n",
    "        faiss_index = indices[0][0]\n",
    "        cached_query = self.cache_base.get(f\"faiss_index:{faiss_index}\")\n",
    "\n",
    "        if distances[0][0] >= similarity_threshold and cached_query:\n",
    "            cached_response = self.cache_base.get(cached_query)\n",
    "            self.total_hits += 1\n",
    "            query_tokens = self.__get_token_count(query)\n",
    "            response_tokens = self.__get_token_count(cached_response)\n",
    "            self.saved_tokens[0] += query_tokens\n",
    "            self.saved_tokens[1] += response_tokens\n",
    "            return cached_response\n",
    "\n",
    "        response = self.llm.generate(query)\n",
    "\n",
    "        if self.save and self.counter <= self.cache_size:\n",
    "            # self.add_to_cache(query, response)\n",
    "            if self.cluster_queries(query,response):\n",
    "              self.add_to_cache(query, response)\n",
    "              # self.cache_base.delete(f\"faiss_index:{self.counter - 1}\")\n",
    "              # self.vector_base.index.remove_ids(np.array([self.counter - 1]))\n",
    "              # self.counter -= 1\n",
    "\n",
    "        return response\n",
    "\n",
    "    def populate_cache(self, queries: list[str], responses: list[str]):\n",
    "        for query, response in zip(queries, responses):\n",
    "            self.add_to_cache(query, response)\n",
    "\n",
    "    def cluster_queries(self,query,response):\n",
    "        if self.counter < self.min_samples:\n",
    "            return True\n",
    "\n",
    "        embeddings = []\n",
    "        saved_tokens = []\n",
    "\n",
    "\n",
    "        for index in range(self.counter):\n",
    "            query = str(self.cache_base.get(f\"faiss_index:{index}\"))\n",
    "            response = str(self.cache_base.get(query))\n",
    "            saved_tokens.append(self.__get_token_count(query) + self.__get_token_count(response))\n",
    "            embeddings.append(self.embedder.embed([query]).flatten())\n",
    "\n",
    "        embedded_query = self.embedder.embed([query])\n",
    "        embeddings.append(embedded_query.flatten())\n",
    "        saved_tokens.append(self.__get_token_count(query)+self.__get_token_count(response))\n",
    "\n",
    "        embeddings = np.array(embeddings)\n",
    "        saved_tokens = np.array(saved_tokens)\n",
    "\n",
    "        dbscan = DBSCAN(eps=self.eps, min_samples=self.min_samples)\n",
    "        clusters = dbscan.fit_predict(embeddings)\n",
    "        unique_clusters = np.unique(clusters)\n",
    "\n",
    "        for cluster in unique_clusters:\n",
    "            cluster_indices = np.where(clusters == cluster)[0]\n",
    "            cluster_proportion = len(cluster_indices) / len(clusters)\n",
    "            saved_tokens_ratio = saved_tokens[cluster_indices].sum()\n",
    "\n",
    "            if cluster_proportion < self.Tp or saved_tokens_ratio > self.Ts:\n",
    "                return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPjFYf0qUiZ0"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1z6gcZm7iovi",
    "outputId": "7eb38b73-f0f2-4480-ccfb-34e1ced0f21c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cache_base = CacheBase()\n",
    "embedder = Embedder()\n",
    "vector_base = VectorBase(dimension= embedder.dimension)\n",
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00N8YyNjnSoM"
   },
   "outputs": [],
   "source": [
    "cache_manager = CacheManager(cache_base,vector_base,embedder,llm,save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PArHk4gdX2oO"
   },
   "source": [
    "# Clear The Cache Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQnnGiahjArK"
   },
   "outputs": [],
   "source": [
    "cache_manager.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVJHX5ZHYTaw"
   },
   "source": [
    "# Number of the cached data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqyibsS3fipz",
    "outputId": "8c3af066-2bdb-475e-ecfa-fdc839d3023a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_manager.counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwsc9HlTYQBI"
   },
   "source": [
    "# Hit ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbYxQOS6Q5Hg",
    "outputId": "734a2c93-1514-427a-df8e-7989beed01ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cache_manager.total_hits/cache_manager.counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to3TB_Z2v5zb"
   },
   "source": [
    "# Total number of saved tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjkD8Sn2vVUS",
    "outputId": "417fe2dc-45fb-48a2-e055-2bd150ec6801"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_manager.saved_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "RRFuh9w1xw0Z",
    "outputId": "a2d9c782-0564-4a76-ac37-a449d815456a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83.9 ms, sys: 32.7 ms, total: 117 ms\n",
      "Wall time: 1.07 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cache_manager.get_from_cache(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LH3RJA1FygDD",
    "outputId": "48a17987-bfa9-4a4b-b5f3-68e2485a3aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 ms, sys: 949 µs, total: 19.4 ms\n",
      "Wall time: 21.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'The capital of France is Paris.'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cache_manager.get_from_cache(\"Where the capital of France is located?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "4hLHhozuchIV",
    "outputId": "81fbedd8-93fe-4f13-d6ee-f6b0e08faef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.5 ms, sys: 4.01 ms, total: 67.5 ms\n",
      "Wall time: 4.28 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"OpenAI is a non-profit artificial intelligence (AI) research organization that aims to ensure that AI is beneficial to humanity. It was founded in 2015 by a group of researchers and entrepreneurs, including Elon Musk, Sam Altman, and others.\\n\\nOpenAI's mission is to:\\n\\n1. **Advance the state of AI**: OpenAI's research focuses on developing and applying AI to help humans learn, work, and create. They aim to push the boundaries of what is possible with AI, while ensuring that the technology is safe, transparent, and beneficial to society.\\n2. **Make AI more accessible**: OpenAI provides open-source AI tools, models, and datasets to the research community, making it easier for others to build upon their work and advance the field.\\n3. **Ensure AI is beneficial to humanity**: OpenAI is committed to ensuring that AI is used for the greater good, rather than for malicious purposes. They work to develop AI systems that are transparent, explainable, and accountable, and that align with human values.\\n\\nSome of OpenAI's notable achievements include:\\n\\n1. **GPT-3**: A large language model that can generate human-like text, answer questions, and even create its own text based on a prompt.\\n2. **DALL-E**: A model that can generate images from text prompts, such as creating images of objects, scenes, and even entire stories.\\n3. **CLIP**: A model that can understand and generate natural language, and can be used for tasks like image captioning, text-to-image synthesis, and more.\\n\\nOpenAI's work has far-reaching implications for various industries, including healthcare, education, finance, and more. Their research and innovations have the potential to transform the way we live, work, and interact with each other.\\n\\nWould you like to know more about a specific aspect of OpenAI's work or its applications?\""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cache_manager.get_from_cache(\"What is the openai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnjdGVtPckao",
    "outputId": "2c7602b3-1b71-4b7d-e4db-cbe526652839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 ms, sys: 0 ns, total: 18.9 ms\n",
      "Wall time: 20.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b\"OpenAI is a non-profit artificial intelligence (AI) research organization that aims to ensure that AI is beneficial to humanity. It was founded in 2015 by a group of researchers and entrepreneurs, including Elon Musk, Sam Altman, and others.\\n\\nOpenAI's mission is to:\\n\\n1. **Advance the state of AI**: OpenAI's research focuses on developing and applying AI to help humans learn, work, and create. They aim to push the boundaries of what is possible with AI, while ensuring that the technology is safe, transparent, and beneficial to society.\\n2. **Make AI more accessible**: OpenAI provides open-source AI tools, models, and datasets to the research community, making it easier for others to build upon their work and advance the field.\\n3. **Ensure AI is beneficial to humanity**: OpenAI is committed to ensuring that AI is used for the greater good, rather than for malicious purposes. They work to develop AI systems that are transparent, explainable, and accountable, and that align with human values.\\n\\nSome of OpenAI's notable achievements include:\\n\\n1. **GPT-3**: A large language model that can generate human-like text, answer questions, and even create its own text based on a prompt.\\n2. **DALL-E**: A model that can generate images from text prompts, such as creating images of objects, scenes, and even entire stories.\\n3. **CLIP**: A model that can understand and generate natural language, and can be used for tasks like image captioning, text-to-image synthesis, and more.\\n\\nOpenAI's work has far-reaching implications for various industries, including healthcare, education, finance, and more. Their research and innovations have the potential to transform the way we live, work, and interact with each other.\\n\\nWould you like to know more about a specific aspect of OpenAI's work or its applications?\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cache_manager.get_from_cache(\"tell me about openai?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJzk1e36Z8Pp"
   },
   "source": [
    "# Retrieving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eys3hnS9Z10O",
    "outputId": "4036ebd3-7b11-4297-8eae-e7789f1d8c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1t0ojkPMjY1Euaz3Ve66_EVBFFazfKLeK\n",
      "To: /content/similar_quora_general_dataset_final.csv\n",
      "\r  0% 0.00/7.53M [00:00<?, ?B/s]\r100% 7.53M/7.53M [00:00<00:00, 110MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1t0ojkPMjY1Euaz3Ve66_EVBFFazfKLeK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLLDEglhaLS_"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('similar_quora_general_dataset_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7v9yHqiyaZBp",
    "outputId": "c5a9aa94-0a88-4acf-987e-6ee33cf95cfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.2  10000 non-null  int64 \n",
      " 1   Unnamed: 0.1  10000 non-null  int64 \n",
      " 2   Unnamed: 0    10000 non-null  int64 \n",
      " 3   id            10000 non-null  int64 \n",
      " 4   qid1          10000 non-null  int64 \n",
      " 5   qid2          10000 non-null  int64 \n",
      " 6   question1     10000 non-null  object\n",
      " 7   question2     10000 non-null  object\n",
      " 8   is_duplicate  10000 non-null  int64 \n",
      " 9   answer        10000 non-null  object\n",
      "dtypes: int64(7), object(3)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AO6q_FbHabqo"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0.2',\t'Unnamed: 0.1',\t'Unnamed: 0',\t'id',\t'qid1',\t'qid2'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "EoJWDd_vbRId",
    "outputId": "13faf97a-8525-448e-ed0d-25c4a8c0ccb0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"question1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9075,\n        \"samples\": [\n          \"Why do people still think the the world is flat?\",\n          \"How is the life in North Korea?\",\n          \"Who is your favourite non-fiction author?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9116,\n        \"samples\": [\n          \"People says that I lack imagination when I clearly not, what should do? How should I response to them?\",\n          \"How do I get investors for a business?\",\n          \"How do I know if I'm being manipulated?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_duplicate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9954,\n        \"samples\": [\n          \" Gaining muscle mass or overall bodyweight requires regular calorie surplus, which means you need to consume more calories than your current intake. However, it's important that the additional calories come from nutrient-dense foods such as lean proteins (chicken breast or turkey), complex carbohydrates like whole grains and vegetables.\\n  Here are some tips: 1) Eat frequently - aim for 6 small meals per day instead of three large ones; this helps maintain metabolism high throughout the entirety of the waking hours;  2). Include lean protein sources with every meal - try combining carbs with lean proteines like Greek yogurt or cottage cheese every single time so it becomes second nature eventually ;  3.) Incorporate strength training exercises into routine - these will help build muscle tissue while burning fat simultaneously ;  4.) Be sure not overdo any one thing - if trying hard but not seeing results then chances are something is off so make changes accordingly ;  5 ) Finally be patient! It takes time for bodies change so try focusing on progress rather than total end result healthily!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-62a094df-38dd-4f76-96fe-803e8a121e4b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is it like to be undergraduate students?</td>\n",
       "      <td>What is it like to be an undergraduate student?</td>\n",
       "      <td>1</td>\n",
       "      <td>Being an Undergrad student can vary greatly d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Money: What would a world without money be like?</td>\n",
       "      <td>What would the world be like if money didn't e...</td>\n",
       "      <td>1</td>\n",
       "      <td>A world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some Punjabi jokes?</td>\n",
       "      <td>What are some good Punjabi jokes?</td>\n",
       "      <td>1</td>\n",
       "      <td>1. Why don't you ever see a bald Sikh man at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How was the International Space Station built?</td>\n",
       "      <td>How was international space station made?</td>\n",
       "      <td>1</td>\n",
       "      <td>The construction of theInternationalSpaceStat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the best Skullcandy earbuds?</td>\n",
       "      <td>What are the best Skullcandy earbuds and headp...</td>\n",
       "      <td>1</td>\n",
       "      <td>The \"best\" skull candy headphones may vary de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62a094df-38dd-4f76-96fe-803e8a121e4b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-62a094df-38dd-4f76-96fe-803e8a121e4b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-62a094df-38dd-4f76-96fe-803e8a121e4b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-a77d0070-58eb-406d-96a8-fe1ee788dea2\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a77d0070-58eb-406d-96a8-fe1ee788dea2')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-a77d0070-58eb-406d-96a8-fe1ee788dea2 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          question1  \\\n",
       "0     What is it like to be undergraduate students?   \n",
       "1  Money: What would a world without money be like?   \n",
       "2                      What are some Punjabi jokes?   \n",
       "3    How was the International Space Station built?   \n",
       "4             What are the best Skullcandy earbuds?   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0    What is it like to be an undergraduate student?             1   \n",
       "1  What would the world be like if money didn't e...             1   \n",
       "2                  What are some good Punjabi jokes?             1   \n",
       "3          How was international space station made?             1   \n",
       "4  What are the best Skullcandy earbuds and headp...             1   \n",
       "\n",
       "                                              answer  \n",
       "0   Being an Undergrad student can vary greatly d...  \n",
       "1                                            A world  \n",
       "2   1. Why don't you ever see a bald Sikh man at ...  \n",
       "3   The construction of theInternationalSpaceStat...  \n",
       "4   The \"best\" skull candy headphones may vary de...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THoBad1Pv_-d"
   },
   "source": [
    " Population the cache storage with question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Riqs2mwpbTO1"
   },
   "outputs": [],
   "source": [
    "cache_manager.populate_cache(df['question1'],df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDvrXxsgwJOh"
   },
   "source": [
    "Evaluating the embedding model performance on question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67ftEt4OfrCC"
   },
   "outputs": [],
   "source": [
    "for query in df['question2']:\n",
    "  cache_manager.get_from_cache(query)\n",
    "  print(cache_manager.total_hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W46es6PmJAto"
   },
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJ1UHQBIIpuw",
    "outputId": "c3f292fb-5014-4072-94b0-04f30af9134c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/praneshmukhopadhyay/amazon-questionanswer-dataset\n",
      "License(s): CC0-1.0\n",
      "Downloading amazon-questionanswer-dataset.zip to /content\n",
      "100% 424M/426M [00:19<00:00, 25.8MB/s]\n",
      "100% 426M/426M [00:19<00:00, 23.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download praneshmukhopadhyay/amazon-questionanswer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iI947uEpIu6h"
   },
   "outputs": [],
   "source": [
    "unzipped_file_path = \"/content/amazon-questionanswer-dataset.zip\"\n",
    "!unzip -q {unzipped_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLDsf-dqIybO"
   },
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv(\"/content/multi_questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMNndDcuIz-i"
   },
   "outputs": [],
   "source": [
    "df_answers = pd.read_csv(\"/content/multi_answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od3GDgU5J7KE"
   },
   "source": [
    "# Warming up the KNN classifier and CacheManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6zsNDQqL78n",
    "outputId": "11c788d5-c575-414f-db8d-28cb474721e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-5202a76a300e>:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_queries_with_answers = merged_df.groupby('Category').apply(lambda x: x.sample(n=n_samples_per_category, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(df_questions, df_answers, on='QuestionID', how='inner')\n",
    "\n",
    "n_samples_per_category = 10\n",
    "\n",
    "sampled_queries_with_answers = merged_df.groupby('Category').apply(lambda x: x.sample(n=n_samples_per_category, random_state=42))\n",
    "\n",
    "sampled_queries_with_answers = sampled_queries_with_answers.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vd6MCmhNO7qy"
   },
   "outputs": [],
   "source": [
    "cache_warmup, test = train_test_split(sampled_queries_with_answers, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tbKmG7smzFO"
   },
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkJVPlrbmx8X"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pre_processing1(query: str):\n",
    "\n",
    "    query = query.lower()\n",
    "\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "    query = re.sub(r'\\d+', '', query)\n",
    "\n",
    "    query = ' '.join(query.split())\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GckExqCkzT67"
   },
   "outputs": [],
   "source": [
    "assistant_content = \"\"\"You are a text preprocessing assistant. Your task is to clean and prepare the given queries for further evaluation. Do not provide answers or additional information—only process the text to make it clearer and free from errors.\n",
    "\n",
    "              Please follow these steps:\n",
    "              1. Convert the text to lowercase.\n",
    "              2. Remove any special characters, punctuation.\n",
    "              3. Remove common stopwords (if applicable).\n",
    "              4. Dont change the meaning of the sentence.\n",
    "              5. Return the cleaned text as a single string without any additional comments or outputs.\n",
    "\n",
    "              The input is delimited by < >.\n",
    "              \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vH3qJ1zqvwt"
   },
   "outputs": [],
   "source": [
    "from together import Together\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "def pre_processing(query: str):\n",
    "\n",
    "    client = Together(api_key=\"\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-Vision-Free\",\n",
    "        messages=[\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "            {\"role\": \"user\", \"content\": f\"<{query}>\"}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QmgocQqwnbk"
   },
   "outputs": [],
   "source": [
    "confusing_queries = [\n",
    "    \"Whre th captiol of Frnace is locatd?\",\n",
    "    \"Wha t ime do the stors cloes?\",\n",
    "    \"Cna you tell me whts the weathr like in New Yrok?\",\n",
    "\n",
    "    \"Where the captial of Fraunce iz?\",\n",
    "    \"How many pple liv in the UK?\",\n",
    "    \"What ar the best rstrnts in Pariss?\",\n",
    "\n",
    "    \"What is the capital of France?!#@?\",\n",
    "    \"How many countries??!! are there in Europe??\",\n",
    "    \"What is the time???!! in Tokyo???\",\n",
    "\n",
    "    \"Where is the capital of France is located?\",\n",
    "    \"What is the bestest way to learn coding?\",\n",
    "    \"How much does it cost for the tickets for concert?\",\n",
    "\n",
    "    \"Dove si trova la capitale di France?\",\n",
    "    \"Where is the Hauptstadt of Germany?\",\n",
    "    \"Qual é o preço do bilhete para o cinema?\",\n",
    "\n",
    "    \"Can you please tell me what is the capital city of France?\",\n",
    "    \"I would like to know what time it is now at this moment in New York.\",\n",
    "    \"What is the current weather condition right now in Paris?\",\n",
    "\n",
    "    \"What’s the thing with the Eiffel Tower?\",\n",
    "    \"Tell me about the place where people go to see shows.\",\n",
    "    \"What do you think about the food in that one country?\",\n",
    "\n",
    "    \"Can you tell me, with a detailed explanation, about the capital of France, including its history, culture, and what it’s like to live there?\",\n",
    "    \"What is the name of the city that is known for its art, cuisine, and history, particularly in relation to a famous landmark known as the Eiffel Tower?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEOqBIaantBl"
   },
   "outputs": [],
   "source": [
    "for query in confusing_queries:\n",
    "  print(pre_processing(query))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OJzk1e36Z8Pp",
    "W46es6PmJAto",
    "Od3GDgU5J7KE",
    "-tbKmG7smzFO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
